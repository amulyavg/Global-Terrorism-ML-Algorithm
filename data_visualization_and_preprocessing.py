# -*- coding: utf-8 -*-
"""Global Terrorism ML Algorithm.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1N-dhbO_V9XqUmEM046TkEl4nv-f8ju24
"""

!unzip terrorismdata.zip

import pandas as pd
import numpy as np

fpath = '/content/globalterrorismdb_0718dist.csv'
data = pd.read_csv(fpath, encoding='ISO-8859-1')
data_copy = data.copy()
df = data.copy()
data

data.replace([float('inf'), float('-inf')], pd.NA, inplace=True)

data.drop(index=data.index[0], axis=0, inplace=True) # remove first row of data
data.info()
data.columns
data.describe()

data.isnull().sum()

print('The dataset has {} rows and {} columns'.format(data.shape[0], data.shape[1]))

print(data.describe())
print(data.head())
print(data.dtypes.value_counts())

print("There are {} missing values in this dataset".format(data.isnull().sum().sum()))
print("Number of rows = %d"%(data.shape[0]))
print("Number of columns = {}".format(data.shape[1]))

print("Number of missing values:")
for col in data.columns:
    print('\t%s:%d' % (col,data[col].isna().sum()))

data_modified = data.copy()

#Removal of columns containing missing values

threshold = 0.5  # 50% threshold for missing values i.e, more than 127182.3

# Calculate the percentage of missing values for each column
missing_percentages = data_modified.isnull().mean()

# Filter out columns with missing percentages exceeding the threshold
columns_to_drop = missing_percentages[missing_percentages > threshold].index

# Drop the columns from the DataFrame
data_modified.drop(columns=columns_to_drop, inplace=True)

print("There are {} missing values in this dataset".format(data_modified.isnull().sum().sum()))
no_of_col = 0
for col in data_modified.columns:
    no_of_col = no_of_col +1
    print('\t%s: %d' % (col,data_modified[col].isna().sum()))
print(no_of_col)

print(data_modified.shape)

print("Number of rows = %d"%(data_modified.shape[0]))
print("Number of columns = {}".format(data_modified.shape[1]))

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

# import data
df = pd.read_csv('/content/globalterrorismdb_0718dist.csv', low_memory=False, encoding='latin-1')
df.head()

# create a subset of the data to answer the question "Which attack types are most correlated with which regions?"

df_attack_region = df[['region_txt', 'attacktype1_txt']]

df_attack_region.head()

# convert the subset to binary

# Convert 'region_txt' column to binary columns
df_attack_region = pd.get_dummies(df_attack_region, columns=['region_txt', 'attacktype1_txt'])

# Format column names
df_attack_region.rename(columns=lambda x: x.split('_')[-1], inplace=True)

# Changing "False" to "0" and "True" to "1"
df_attack_region = df_attack_region.replace({False: 0, True: 1})

df_attack_region.head()

# Conduct ARM with a minimum support of 10%

from mlxtend.frequent_patterns import apriori, association_rules
import matplotlib.pyplot as plt

from mlxtend.frequent_patterns import apriori, association_rules
import matplotlib.pyplot as plt


freq_attack_region = apriori(df_attack_region, min_support=0.1, use_colnames=True, verbose=1)

freq_attack_region.head(30)

# Association rules for itemsets with a minimum confidence of 40%

rules = association_rules(freq_attack_region, metric="confidence", min_threshold=0.4)
rules.head()

# Conduct ARM with a minimum support of 2%

from mlxtend.frequent_patterns import apriori, association_rules
import matplotlib.pyplot as plt

from mlxtend.frequent_patterns import apriori, association_rules
import matplotlib.pyplot as plt


freq_attack_region = apriori(df_attack_region, min_support=0.02, use_colnames=True, verbose=1)

freq_attack_region.head(30)

# Association rules for itemsets with a minimum confidence of 30%

rules = association_rules(freq_attack_region, metric="confidence", min_threshold=0.3)
rules.head(10)

import matplotlib.pyplot as plt

# Data
attack_types = ['Bombing/Explosion', 'Armed Assault', 'Assassination', 'Hostage Taking (Kidnapping)', 'Facility/Infrastructure Attack']
support_values = [0.485742, 0.234844, 0.106290, 0.061412, 0.056998]

# Calculate support values for "other" category
other_support = 1 - sum(support_values)

# Append "other" category to the lists
attack_types.append('Other')
support_values.append(other_support)

# Plot
plt.figure(figsize=(10, 6))
bars = plt.barh(attack_types, support_values, color='skyblue')
plt.xlabel('Support Value')
plt.ylabel('Attack Type')
plt.title('Most Frequently Occurring Terrorist Attack Type')
plt.gca().invert_yaxis()  # Invert y-axis to have the highest support at the top

# Add percentages on the bars
for bar in bars:
    width = bar.get_width()
    plt.text(width - 0.02, bar.get_y() + bar.get_height() / 2, f'{width*100:.1f}%', ha='center', va='center', color='black')

plt.show()

# Create a pie chart showing the frequency of regions

import matplotlib.pyplot as plt

# Data
regions = ['Middle East & North Africa', 'South Asia', 'South America', 'Sub-Saharan Africa', 'Western Europe', 'Southeast Asia', 'Central America & Caribbean', 'Other']
percentages = [27.7, 24.7, 10.4, 9.6, 9.1, 6.8, 5.6, 6.1]

# Plot
plt.figure(figsize=(8, 8))
plt.pie(percentages, labels=regions, autopct='%1.1f%%', startangle=140)
plt.title('Frequency of Regions in Global Terrorism Database')
plt.show()

import matplotlib.pyplot as plt
import seaborn as sns
from sklearn import datasets
import pandas as pd

df = pd.read_csv('/content/globalterrorismdb_0718dist.csv', encoding='ISO-8859-1')

numeric_df = df.select_dtypes(include=['number'])

print("Number of rows = %d"%(data_modified.shape[0]))
print("Number of columns = {}".format(data_modified.shape[1]))

corr_matrix = numeric_df.corr()

# Create a heatmap using seaborn
plt.figure(figsize=(40, 35))
sns.heatmap(corr_matrix, annot=True, cmap='Greens', linewidths=0.5)
plt.title('Correlation Matrix')
plt.show()

import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd

# Load the dataset
df = pd.read_csv('/content/globalterrorismdb_0718dist.csv', encoding='ISO-8859-1')

# Filter numeric columns
numeric_df = df.select_dtypes(include=['number'])

# Calculate the correlation matrix
corr_matrix = numeric_df.corr()

# Set the correlation threshold
threshold = 0.85

# Identify pairs of highly correlated columns
corr_pairs = corr_matrix.abs().unstack().sort_values(kind="quicksort", ascending=False)

# Remove self-correlations (correlation of a column with itself)
corr_pairs = corr_pairs[corr_pairs != 1]

# Create a set to store columns to be removed
columns_to_remove = set()

# Loop through the correlation pairs and identify columns to remove
for i in range(0, len(corr_pairs), 2):
    if corr_pairs[i] > threshold:
        col1 = corr_pairs.index[i][0]
        col2 = corr_pairs.index[i][1]
        if col1 not in columns_to_remove:
            columns_to_remove.add(col2)

# Print the columns to be removed
print(f"Columns to be removed: {columns_to_remove}")

# Remove the identified columns from the DataFrame
df_reduced = numeric_df.drop(columns=columns_to_remove)

# Calculate and print the new correlation matrix
corr_matrix_reduced = df_reduced.corr()
print(corr_matrix_reduced)

# Visualize the reduced correlation matrix
plt.figure(figsize=(50, 35))
sns.heatmap(corr_matrix_reduced, annot=True, cmap='Greens', linewidths=0.5)
plt.title('Reduced Correlation Matrix')
plt.show()

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler

from sklearn.datasets import make_classification

df_reduced = df_reduced.fillna(df_reduced.mean())

# 1. Standardize the data
scaler = StandardScaler()
scaled_data = scaler.fit_transform(df_reduced)

# 2. Perform PCA
pca = PCA()
principal_components = pca.fit_transform(scaled_data)

print("Covariance Matrix:\n", pca.get_covariance(), '\n')
print('\nEigenvalues:', pca.explained_variance_)

print("Explained Variance Ratios:", pca.explained_variance_ratio_, '\n')

total_explained_variances = []
for i in range(len(pca.explained_variance_ratio_)):
  total_explained_variances.append(sum(pca.explained_variance_ratio_[:i + 1]))
  print('Total explained variance including', i + 1, 'components:', total_explained_variances[-1])
  print('We', 'should use more components' if total_explained_variances[-1] < 0.9 else 'do not need to use more components.', '\n')

plt.figure(figsize=(8, 6))
plt.bar(range(1, len(pca.explained_variance_ratio_) + 1), pca.explained_variance_ratio_, alpha=0.5, align='center', label='individual explained variance')
plt.step(range(1, len(total_explained_variances) + 1), total_explained_variances, where='mid', label='total explained variance')

plt.xlabel('Principal Components')
plt.ylabel('Explained Variance Ratio')
plt.legend(loc='best')
plt.title('Explained Variance Ratio and Total Explained Variance Ratio')

plt.grid()
plt.show()

plt.figure(figsize=(8, 6))

plt.plot(range(1, len(pca.explained_variance_) + 1), pca.explained_variance_, marker='o')

plt.xlabel('Principal Component')
plt.ylabel('Eigenvalue')
plt.title('Scree Plot')

plt.grid()
plt.show()

print("Explained Variances:", pca.explained_variance_, '\n')

for i in range(len(pca.explained_variance_)):
  ev = pca.explained_variance_[i]
  print('Component', i + 1, ' has explained variance of:', ev)
  print('We will', 'not' if ev <= 1 else '', 'use this component.\n')

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler

# Assuming df_reduced is already defined and preprocessed
df_reduced = df_reduced.fillna(df_reduced.mean())

# 1. Standardize the data
scaler = StandardScaler()
scaled_data = scaler.fit_transform(df_reduced)

# 2. Perform PCA
pca = PCA()
principal_components = pca.fit_transform(scaled_data)

# 3. Print covariance matrix and eigenvalues
print("Covariance Matrix:\n", np.cov(principal_components, rowvar=False), '\n')
print('Eigenvalues:', pca.explained_variance_, '\n')
print('Explained Variance Ratio:', pca.explained_variance_ratio_, '\n')

# 4. Determine which components to use based on explained variance
for i in range(len(pca.explained_variance_)):
    ev = pca.explained_variance_[i]
    print('Component', i + 1, 'has explained variance of:', ev)
    if ev <= 1:
        print('We will not use this component.\n')
    else:
        print('We will use this component.\n')

# 5. Plot the explained variance
plt.figure(figsize=(10, 7))
plt.plot(np.cumsum(pca.explained_variance_ratio_), marker='o', linestyle='--')
plt.xlabel('Number of Components')
plt.ylabel('Cumulative Explained Variance')
plt.title('Explained Variance by PCA Components')
plt.show()

# If you want to save the principal components and the variance ratio to a CSV file for further use:
pca_df = pd.DataFrame(principal_components, columns=[f'PC{i+1}' for i in range(principal_components.shape[1])])
pca_df.to_csv('pca_transformed_data.csv', index=False)

print("PCA transformation complete and saved to CSV.")

